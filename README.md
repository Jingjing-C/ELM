# Extreme-Learning-Machine method

一、基本思想
极限学习机(Extreme Learning Machine,ELM)由Huang[1]等人提出的用于求解单隐层神经网络的算法，其特点是：（1）输入权重和隐含层偏置均随机产生，
且设定后可不再调整；（2）隐含层和输出层之间的连接权值不需要迭代，通过解方程式一次性产生。基于以上特点，ELM对于传统的神经网络，尤其是单隐层前
馈神经网络(SLFNs)，在保证学习精度的前提下比传统的学习算法速度更快。

二、算法描述
假设一组N个不同的样本(X_i,t_i)，其中X_i=[x_i1,x_i2,...,x_in ]^T∈R^n，t_i=[t_i1,t_i2,...,t_im ]^T∈R^m，
对于一个有L个隐层节点的单隐层神经网络可以表示为：∑_(i=1)^L▒〖β_i h(a_i,b_i,X_j)=o_j,j=1,...,N〗
其中，a_i和β_i分别是输入权重和输出权重，b_i是第i个隐层单元的偏置，h(•)是激活函数。
许多非线性激活函数可用于WELM，包括sigmoid函数，正弦函数，relu函数等。
当估计输出o_j和实际输出t_i之间的误差为零，SLFNs可以准确地逼近数据的特征，可以表示为：
          ∑_(i=1)^L▒〖β_i g(a_i,b_i,X_j)=t_j,j=1,...,N〗	 
给定一组用于二分类的训练样本(X_i,t_i ),i=1,…,N。
其中X_i=[x_i1,x_i2,...,x_in ]^T∈R^n，t_i=[t_i1,t_i2,...,t_im ]^T∈R^m。
定义与训练样本相关的N×N维对角矩阵W。一般来说，若训练数据X_i来自于少数类，那么对应的权重W_ii也会相对较大。具有L个隐含层节点得极限学习机可以定义为：
∑_(i=1)^L▒〖β_i h(a_i,b_i,X_j)=o_j,j=1,...,N〗	
其中o_j表示预测输出。
作为加权极限学习机参数，a_i表示输入权重矩阵，β_i表示输出权重矩阵。
隐含层节点函数h(a_i,b_i,X_j)包括几乎所有的非线性激活函数，如sigmoid函数、高斯函数、Relu函数等。
当预测输出与实际输出的差值为零时，加权极限学习机可以准确的估计数据的特征：
i=1Lβih(a_i,b_i,X_j)=t_j,j=1,...,N

为了使二分类的分类边界达到最大的同时能够使加权累计误差最小，加权极限学习机的优化问题可以写为：
minimize:‖Hβ-T‖^2 and ‖β‖^	
其中β=[β_1^T,...,β_L^T ]^T，T=[T_1^T,...,T_N^T ]^T，H是隐含层节点的输出，写为:
H=[■(h(a_1,b_1,X_1)&⋯&h(a_L,b_L,X_1)@⋮&⋯&⋮@h(a_1,b_1,X_N)&⋯&h(a_L,b_L,X_N))]_(N×L)	
更确切的表达为：
minimize∶1/2 ‖β‖^2+CW 1/2 ∑_(j=1)^N▒‖ξ_j ‖^2 

subject to∶h(a_i,b_i,X_j )β= t_j^T-ξ_j^T

其中ξ_j^T是样本X_j的训练误差，由期望输出t_j和实际输出h(a_i,b_i,X_j )β做差产生。
加权极限学习机的输入权重a_i和隐含层阈值b_i是训练过程中随机产生的，输出权重经拉格朗日KKT算子求解可得：
β ̂=H^† T={█(〖(I/C+H^T WH)〗^(-1) H^T WT,L<N@H^T 〖(I/C+WHH^T)〗^(-1) WT,L≥N)┤	
其中H^†表示H的Moore-Penrose广义逆。



